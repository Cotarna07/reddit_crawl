import os
import re
import json
import time
import datetime
import requests
import praw
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm  # éœ€è¦ pip install tqdm

# ========== Reddit å‡­è¯ ==========
reddit = praw.Reddit(
    client_id='Zr33iE9lsTMwy83gS7dJkg',
    client_secret='AbntaoukmjaBf0qR_zpW52fvvAjLLA',
    user_agent='python:chen musheng:v1.0 (by /u/CMS_Ducking)',
)

# ========== å¸¸é‡é…ç½® ==========
LINKS_JSON = "links_store.json"    # ä»è¿™é‡Œè¯»å–è¦ä¸‹è½½çš„å¸–å­åˆ—è¡¨
OUTPUT_ROOT = "downloaded_posts"   # åœ¨æ­¤ç›®å½•ä¸‹ï¼Œä¸ºæ¯ä¸ªsubredditåˆ›å»ºå­ç›®å½•ï¼Œå†åœ¨å…¶ä¸­æŒ‰post_idå­˜æ”¾
MAX_WORKERS = 5                    # å¹¶å‘ä¸‹è½½åª’ä½“çš„çº¿ç¨‹æ•°

# ========== 1. åŠ è½½/ä¿å­˜é“¾æ¥åˆ—è¡¨ ==========

def load_links_store():
    """ä» links_store.json åŠ è½½å¸–å­è®°å½•"""
    if not os.path.exists(LINKS_JSON):
        return []
    try:
        with open(LINKS_JSON, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        return []

def save_links_store(data_list):
    """å°†æ›´æ–°åçš„å¸–å­ä¿¡æ¯å†™å› links_store.json"""
    with open(LINKS_JSON, "w", encoding="utf-8") as f:
        json.dump(data_list, f, ensure_ascii=False, indent=2)


# ========== 2. æå–åª’ä½“é“¾æ¥ç›¸å…³å‡½æ•° ==========

def extract_media_links(text):
    """
    ä»è¯„è®ºæ–‡æœ¬ä¸­è¯†åˆ«å„ç§åª’ä½“é“¾æ¥ï¼ˆæ™®é€šå›¾ç‰‡ã€gifvã€giphyï¼‰ã€‚
    å¯æ ¹æ®å®é™…éœ€æ±‚æ‰©å……/ä¿®æ”¹ã€‚
    """
    media_links = []
    # å¸¸è§å›¾ç‰‡ (jpgã€pngã€gif)ï¼ŒåªåŒ¹é… reddit åŸŸåçš„æƒ…å†µ:
    pattern_img = r'(https?://(?:i\.redd\.it|v\.redd\.it|[\w\.]*reddit\.com)/[^\s]+(?:\.jpg|\.png|\.gif))'
    media_links.extend(re.findall(pattern_img, text))

    # gifv
    pattern_gifv = r'(https?://[^\s]+\.gifv)'
    media_links.extend(re.findall(pattern_gifv, text))

    # giphy => å½¢å¦‚ "giphy|xxxx"
    pattern_giphy = r'(giphy\|[a-zA-Z0-9]+)'
    media_links.extend(re.findall(pattern_giphy, text))

    return media_links


# ========== 3. åª’ä½“ä¸‹è½½å‡½æ•° + ä¸»è´´ä¸‹è½½åˆ¤å®š ==========

downloaded_links = set()  # è®°å½•å·²ä¸‹è½½è¿‡çš„é“¾æ¥ï¼Œé¿å…é‡å¤

def download_file(url, local_path):
    """
    çœŸæ­£çš„ä¸‹è½½é€»è¾‘ï¼Œå¸¦ç®€å•é‡å¤åˆ¤æ–­åŠå¼‚å¸¸å¤„ç†ã€‚
    """
    try:
        if url in downloaded_links:
            return None  # å·²ä¸‹è½½è¿‡
        headers = {'User-Agent': 'Mozilla/5.0'}
        with requests.get(url, stream=True, headers=headers, timeout=10) as r:
            r.raise_for_status()
            with open(local_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        downloaded_links.add(url)
        print(f"[INFO] ä¸‹è½½å®Œæˆ: {local_path}")
        return local_path
    except Exception as e:
        print(f"[ERROR] ä¸‹è½½å¤±è´¥: {url} => {e}")
        return None

def download_image(url, folder):
    """ä¸‹è½½æ™®é€šå›¾ç‰‡ (jpg, png, gif)"""
    filename = os.path.basename(urlparse(url).path)
    local_path = os.path.join(folder, filename)
    return download_file(url, local_path)

def download_gifv(url, folder):
    """ä¸‹è½½ Reddit GIF åŠ¨å›¾ (gifv -> mp4)"""
    video_url = url.replace('.gifv', '.mp4')
    filename = os.path.basename(urlparse(video_url).path)
    local_path = os.path.join(folder, filename)
    return download_file(video_url, local_path)

def download_giphy(giphy_str, folder):
    """ä¸‹è½½ Giphy åŠ¨å›¾ (giphy|<gif_id>)"""
    _, gif_id = giphy_str.split('|', 1)
    gif_url = f"https://media.giphy.com/media/{gif_id}/giphy.gif"
    filename = f"{gif_id}.gif"
    local_path = os.path.join(folder, filename)
    return download_file(gif_url, local_path)

def download_all_media(media_links, folder):
    """
    å¹¶å‘ä¸‹è½½ä¸€ç»„åª’ä½“é“¾æ¥åˆ°æŒ‡å®š folder ç›®å½•ã€‚è¿”å› {url: local_path æˆ– None}
    """
    results = {}
    os.makedirs(folder, exist_ok=True)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {}
        for url in media_links:
            if url.endswith('.gifv'):
                future = executor.submit(download_gifv, url, folder)
            elif url.startswith('giphy|'):
                future = executor.submit(download_giphy, url, folder)
            else:
                future = executor.submit(download_image, url, folder)
            future_to_url[future] = url

        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                filepath = future.result()
                results[url] = filepath
            except Exception as e:
                print(f"[ERROR] å¹¶å‘ä¸‹è½½å‡ºé”™: {url} => {e}")
                results[url] = None

    return results

def maybe_download_main_media(submission, folder):
    """
    å¦‚æœä¸»è´´ url æ˜¯å¯ä¸‹è½½çš„å›¾ç‰‡æˆ–åŠ¨å›¾ï¼Œåˆ™å°†å…¶ä¸‹è½½åˆ° folderã€‚
    è¿”å›ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„ or Noneã€‚
    """
    url = submission.url
    # åŸºäºåç¼€ç®€å•åˆ¤æ–­
    if url.endswith(('.jpg', '.png', '.gif')):
        return download_image(url, folder)
    elif url.endswith('.gifv'):
        return download_gifv(url, folder)
    # è‹¥éœ€è¦å¤„ç†æ›´å¤šå½¢å¼ï¼Œå¦‚ mp4ã€giphyã€imgur ç­‰ï¼Œå¯è‡ªè¡Œæ‰©å±•
    return None


# ========== 4. é€’å½’éå†è¯„è®ºï¼Œæ„å»ºè¯„è®ºæ•°æ®ç»“æ„ ==========

def traverse_comments(comment, comments_data, level=0, parent_author=None):
    author_name = str(comment.author) if comment.author else "[deleted]"
    body_text = comment.body
    ups = comment.ups
    media_links = extract_media_links(body_text)

    comments_data.append({
        "level": level,
        "author": author_name,
        "ups": ups,
        "body": body_text,
        "parent_author": parent_author,
        "media_links": media_links
    })

    for reply in comment.replies:
        if isinstance(reply, praw.models.MoreComments):
            continue
        traverse_comments(reply, comments_data, level+1, author_name)


# ========== 5. ä¸‹è½½ä¸€ä¸ªå¸–å­çš„å…¨éƒ¨å†…å®¹åˆ°æœ¬åœ° ==========

def download_post_content(post_info):
    """
    ç»™å®šä¸€æ¡ {'post_id', 'url', 'status', ...} æ•°æ®ï¼Œ
    é€šè¿‡ PRAW è·å–å¸–å­ä¸»ä½“ + æ‰€æœ‰è¯„è®ºï¼Œå¹¶å°†å…¶å†™åˆ° Markdownã€‚
    åŒæ—¶ä¸‹è½½è¯„è®ºé‡Œçš„åª’ä½“æ–‡ä»¶ï¼›
    ä»¥ "downloaded_posts/<subreddit_name>/<post_id>/" ä¸ºåŸºç¡€è·¯å¾„å­˜æ”¾ã€‚
    è¿˜ä¼šæ£€æŸ¥ä¸»è´´æ˜¯å¦æ˜¯å¯ä¸‹è½½çš„å›¾ç‰‡/åŠ¨å›¾ï¼Œå¹¶ä¸‹è½½ã€‚
    """
    post_id = post_info["post_id"]
    url = post_info["url"]

    # 1) ç”¨submissionæ‹¿åˆ°å¸–å­
    submission = reddit.submission(url=url)
    submission.comments.replace_more(limit=None)  # å±•å¼€æ›´å¤šè¯„è®º

    # è·å–æ‰€å±æ¿å—åï¼ˆå½¢å¦‚ 'memes'ï¼‰
    subreddit_name = str(submission.subreddit.display_name)

    # 2) ä¸ºè¯¥å¸–å­åˆ›å»ºç›®å½•: downloaded_posts/<subreddit>/<post_id>/
    post_folder = os.path.join(OUTPUT_ROOT, subreddit_name, post_id)
    os.makedirs(post_folder, exist_ok=True)

    # å‡†å¤‡å­˜æ”¾åª’ä½“çš„å­ç›®å½•
    media_folder = os.path.join(post_folder, "media_files")
    os.makedirs(media_folder, exist_ok=True)

    # 3) å…ˆä¸‹è½½ä¸»è´´ä¸­çš„å›¾ç‰‡/åŠ¨å›¾ï¼ˆå¦‚æœèƒ½ï¼‰
    main_media_path = maybe_download_main_media(submission, media_folder)

    # 4) è·å–è¯„è®ºå¹¶æ„å»ºæ•°æ®
    all_comments = submission.comments.list()
    comments_data = []
    for c in all_comments:
        if isinstance(c, praw.models.MoreComments):
            continue
        traverse_comments(c, comments_data, level=0, parent_author=None)

    # 5) æ”¶é›†æ‰€æœ‰è¯„è®ºåª’ä½“é“¾æ¥å¹¶å¹¶å‘ä¸‹è½½
    all_links = set()
    for item in comments_data:
        for m_link in item["media_links"]:
            all_links.add(m_link)

    download_map = download_all_media(list(all_links), media_folder)

    # 6) å†™å¸–å­å†…å®¹åˆ° Markdown
    md_file_path = os.path.join(post_folder, f"{post_id}.md")
    with open(md_file_path, "w", encoding="utf-8") as f:
        # å¸–å­å¤´éƒ¨ä¿¡æ¯
        f.write(f"# Reddit Post - {submission.title}\n\n")
        f.write(f"- **Subreddit**: r/{subreddit_name}\n")
        f.write(f"- **Author**: {submission.author}\n")
        f.write(f"- **Ups**: {submission.ups}\n")
        f.write(f"- **URL**: {url}\n\n")
        if submission.selftext:
            f.write(f"**Body**:\n{submission.selftext}\n\n")

        # å¦‚æœä¸»è´´çš„å›¾ç‰‡/åŠ¨å›¾å·²ä¸‹è½½æˆåŠŸï¼Œåˆ™æ’å…¥åˆ° MD
        if main_media_path:
            rel_main_path = os.path.relpath(main_media_path, post_folder).replace("\\", "/")
            f.write(f"**Main Media**:\n")
            f.write(f"![main_media]({rel_main_path})\n\n")

        f.write("---\n\n")
        f.write("## Comments:\n\n")

        # å†™è¯„è®º
        for item in comments_data:
            prefix = "  " * item["level"]
            author = item["author"]
            ups = item["ups"]
            body = item["body"]
            p_author = item["parent_author"]
            m_links = item["media_links"]

            if p_author:
                line = f"{prefix}â†³ **{author}** å›å¤ {p_author} (ğŸ‘ {ups}): {body}\n"
            else:
                line = f"{prefix}**{author}** (ğŸ‘ {ups}): {body}\n"

            # æ’å…¥å¯¹åº”åª’ä½“
            for link in m_links:
                local_path = download_map.get(link)
                if local_path:
                    local_path_fixed = local_path.replace("\\", "/")
                    rel_path = os.path.relpath(local_path_fixed, post_folder).replace("\\", "/")
                    line += f"{prefix}  ![media]({rel_path})\n"

            f.write(line + "\n")

    print(f"[INFO] å¸–å­ {post_id} ({submission.title}) ä¸‹è½½å®Œæˆ => {md_file_path}")


# ========== 6. ä¸»æµç¨‹ï¼šè¯»å–JSONï¼Œä¸‹è½½ "new" å¸–å­ + è¿›åº¦æ¡ ==========

def main():
    data_list = load_links_store()
    # å…ˆç­›é€‰å‡ºæ‰€æœ‰ status=="new" çš„å¸–å­
    new_items = [item for item in data_list if item.get("status") == "new"]
    total_new = len(new_items)

    if total_new == 0:
        print("[INFO] æ²¡æœ‰éœ€è¦ä¸‹è½½çš„å¸–å­ã€‚")
        return

    print(f"[INFO] å‡†å¤‡ä¸‹è½½ {total_new} ä¸ªå¸–å­...")

    # ç”¨ tqdm æ˜¾ç¤ºå¸–å­çº§çš„ä¸‹è½½è¿›åº¦æ¡
    for item in tqdm(new_items, desc="Downloading posts", total=total_new):
        try:
            download_post_content(item)
            # ä¸‹è½½å®Œæˆåï¼Œæ›´æ–°çŠ¶æ€
            item["status"] = "downloaded"
            item["last_update"] = str(datetime.date.today())
        except Exception as e:
            print(f"[ERROR] ä¸‹è½½å¸–å­å¤±è´¥: {item['post_id']} => {e}")

        # ä¸ºäº†é¿å…è¿‡åº¦è¯·æ±‚ï¼Œå¯¹æ¯ä¸ªå¸–å­ç¨ä½œå»¶æ—¶
        time.sleep(2)

    # ä¿å­˜æ›´æ–°åçš„çŠ¶æ€
    save_links_store(data_list)
    print(f"\n[DONE] å·²ä¸‹è½½å¸–å­æ•°: {total_new}")


if __name__ == "__main__":
    main()
