import os
import re
import requests
import praw
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# -----------------------
# 1. Reddit å®¢æˆ·ç«¯åˆå§‹åŒ–
# -----------------------
reddit = praw.Reddit(
    client_id='Zr33iE9lsTMwy83gS7dJkg',
    client_secret='AbntaoukmjaBf0qR_zpW52fvvAjLLA',
    user_agent='python:chen musheng:v1.0 (by /u/CMS_Ducking)',
)

# ç›®æ ‡å¸–å­é“¾æ¥
post_url = 'https://www.reddit.com/r/memes/comments/ig9u4z/she_did_her_best_ok/'
post = reddit.submission(url=post_url)

# åˆ›å»ºä¿å­˜åª’ä½“æ–‡ä»¶çš„æ–‡ä»¶å¤¹
media_dir = 'media_files'
os.makedirs(media_dir, exist_ok=True)

# è·å–å¸–å­è¯„è®º
post.comments.replace_more(limit=None)
comments = post.comments.list()

# ç”¨äºé¿å…é‡å¤ä¸‹è½½
downloaded_links = set()

# Markdown æ–‡ä»¶å
markdown_file = "reddit_comments.md"


# -----------------------
# 2. ä¸‹è½½å‡½æ•°ï¼šä½¿ç”¨æ­£æ–œæ 
# -----------------------
def download_file(url, local_path):
    """çœŸæ­£çš„ä¸‹è½½é€»è¾‘ï¼Œå¯è¢«å…¶ä»–å‡½æ•°å¤ç”¨"""
    try:
        # è‹¥å·²ä¸‹è½½è¿‡åˆ™è·³è¿‡
        if url in downloaded_links:
            return None
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        with requests.get(url, stream=True, headers=headers, timeout=10) as r:
            r.raise_for_status()
            with open(local_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)

        downloaded_links.add(url)
        print(f"[INFO] ä¸‹è½½å®Œæˆ: {local_path}")
        return local_path
    except Exception as e:
        print(f"[ERROR] ä¸‹è½½å¤±è´¥: {url} => {e}")
        return None


def download_image(url):
    """æ™®é€šå›¾ç‰‡ (jpg, png, gif)"""
    filename = os.path.basename(urlparse(url).path)
    local_path = os.path.join(media_dir, filename)
    return download_file(url, local_path)


def download_gifv(url):
    """ä¸‹è½½ Reddit GIF åŠ¨å›¾ (gifv -> mp4)"""
    # ç®€åŒ–å¤„ç†ï¼šç›´æ¥æŠŠ .gifv æ›¿æ¢æˆ .mp4
    video_url = url.replace('.gifv', '.mp4')
    filename = os.path.basename(urlparse(video_url).path)
    local_path = os.path.join(media_dir, filename)
    return download_file(video_url, local_path)


def download_giphy(giphy_str):
    """ä¸‹è½½ Giphy åŠ¨å›¾ (giphy|<gif_id>)"""
    _, gif_id = giphy_str.split('|', 1)
    gif_url = f"https://media.giphy.com/media/{gif_id}/giphy.gif"
    filename = f"{gif_id}.gif"
    local_path = os.path.join(media_dir, filename)
    return download_file(gif_url, local_path)


# -----------------------
# 3. æå–åª’ä½“é“¾æ¥
# -----------------------
def extract_media_links(comment_body):
    """æå–è¯„è®ºä¸­çš„åª’ä½“é“¾æ¥"""
    media_links = []
    # åŒ¹é…å¸¸è§å›¾ç‰‡ (jpgã€pngã€gif)
    pattern_img = r'(https?://(?:i\.redd\.it|v\.redd\.it|[\w\.]*reddit\.com)/[^\s]+(?:\.jpg|\.png|\.gif))'
    media_links.extend(re.findall(pattern_img, comment_body))

    # gifv
    pattern_gifv = r'(https?://[^\s]+\.gifv)'
    media_links.extend(re.findall(pattern_gifv, comment_body))

    # giphy
    pattern_giphy = r'(giphy\|[a-zA-Z0-9]+)'
    media_links.extend(re.findall(pattern_giphy, comment_body))

    return media_links


# -----------------------
# 4. å¹¶å‘ä¸‹è½½
# -----------------------
def download_all_media(media_links):
    results = {}
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_url = {}
        for url in media_links:
            if url.endswith('.gifv'):
                future = executor.submit(download_gifv, url)
            elif url.startswith('giphy|'):
                future = executor.submit(download_giphy, url)
            else:
                future = executor.submit(download_image, url)

            future_to_url[future] = url

        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                filepath = future.result()
                results[url] = filepath
            except Exception as e:
                print(f"[ERROR] å¹¶å‘ä¸‹è½½å‡ºé”™: {url} => {e}")
                results[url] = None
    return results


# -----------------------
# 5. é€’å½’éå†è¯„è®º
# -----------------------
comments_data = []

def traverse_comments(comment, level=0, parent_author=None):
    author_name = str(comment.author) if comment.author else "[deleted]"
    body_text = comment.body
    links = extract_media_links(body_text)

    comments_data.append({
        "level": level,
        "author": author_name,
        "ups": comment.ups,
        "body": body_text,
        "parent_author": parent_author,
        "media_links": links,
    })

    # å¤„ç†å­è¯„è®º
    for reply in comment.replies:
        if isinstance(reply, praw.models.MoreComments):
            continue
        traverse_comments(reply, level + 1, author_name)


# -----------------------
# 6. ä¸»é€»è¾‘
# -----------------------
def main():
    print(f"å¸–å­æ ‡é¢˜: {post.title}")
    print("-" * 40)

    # æ„å»ºè¯„è®ºæ•°æ®
    for c in comments:
        if isinstance(c, praw.models.MoreComments):
            continue
        traverse_comments(c)

    # æ”¶é›†æ‰€æœ‰åª’ä½“é“¾æ¥ï¼Œå»é‡
    all_media_links = set()
    for item in comments_data:
        for l in item["media_links"]:
            all_media_links.add(l)

    # æ‰¹é‡ä¸‹è½½
    download_results = download_all_media(list(all_media_links))

    # å†™å…¥ Markdown
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(f"# Reddit Post: {post.title}\n\n")
        f.write("## Comments:\n\n")

        for item in comments_data:
            level = item["level"]
            prefix = "  " * level
            author = item["author"]
            ups = item["ups"]
            body = item["body"]
            parent = item["parent_author"]
            media_links = item["media_links"]

            if parent:
                comment_line = f"{prefix}â†³ **{author}** å›å¤ {parent} (ğŸ‘ {ups}): {body}\n"
            else:
                comment_line = f"{prefix}**{author}** (ğŸ‘ {ups}): {body}\n"

            # å†™å®Œè¯„è®ºæ­£æ–‡åï¼Œæ’å…¥åª’ä½“
            for m_link in media_links:
                local_path = download_results.get(m_link)
                if local_path:
                    # æ³¨æ„ï¼šè¿™é‡ŒæŠŠ Windows çš„åæ–œæ éƒ½æ›¿æ¢æˆæ­£æ–œæ 
                    local_path_fixed = local_path.replace("\\", "/")
                    comment_line += f"{prefix}  ![media]({local_path_fixed})\n"

            f.write(comment_line + "\n")

    print(f"[DONE] è¯„è®ºå’Œåª’ä½“å·²ä¿å­˜åˆ° {markdown_file}")


if __name__ == "__main__":
    main()
